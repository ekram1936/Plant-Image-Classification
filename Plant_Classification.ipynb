{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Plant Classification.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dwKlVz3RW6K"
      },
      "source": [
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import random\n",
        "import zipfile as zf\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        " \n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import LabelBinarizer,LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        " \n",
        "from keras.models import Sequential,Model\n",
        "from keras.layers import Conv2D,Dense,Flatten,Dropout,AveragePooling2D,MaxPool2D,Input\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.optimizers import Adam,RMSprop,SGD,Nadam,Adamax,Adadelta,Adagrad\n",
        "from keras.applications import ResNet50\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils import np_utils\n",
        " \n",
        "from imutils import build_montages\n",
        " \n",
        "sns.set()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGl07gRyRb84"
      },
      "source": [
        "daruchini_dir = os.listdir('/content/drive/MyDrive/Paper/Dataset/Daruchini')\n",
        "tulsi_dir = os.listdir('/content/drive/MyDrive/Paper/Dataset/Tulsi')\n",
        "tejpata_dir = os.listdir('/content/drive/MyDrive/Paper/Dataset/Tejpata')\n",
        "niim_dir = os.listdir('/content/drive/MyDrive/Paper/Dataset/Niim')\n",
        "pathorkochi_dir = os.listdir('/content/drive/MyDrive/Paper/Dataset/pathorkochi')\n",
        "shojne_dir = os.listdir('/content/drive/MyDrive/Paper/Dataset/shojne')\n",
        "\n",
        "fp_daruchini = '/content/drive/MyDrive/Paper/Dataset/Daruchini/'\n",
        "fp_tulsi= '/content/drive/MyDrive/Paper/Dataset/Tulsi/'\n",
        "\n",
        "fp_tejpata = '/content/drive/MyDrive/Paper/Dataset/Tejpata/'\n",
        "fp_niim = '/content/drive/MyDrive/Paper/Dataset/Niim/'\n",
        "\n",
        "fp_pathorkochi = '/content/drive/MyDrive/Paper/Dataset/pathorkochi/'\n",
        "fp_shojne = '/content/drive/MyDrive/Paper/Dataset/shojne/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ceGgutpUN8f"
      },
      "source": [
        "images = []\n",
        "labels = []\n",
        "\n",
        "for i in daruchini_dir:\n",
        "    if i[-3] != 'n' and i[-3] != 'g':\n",
        "        image = cv2.imread(fp_daruchini+i)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image = cv2.resize(image, (128,128))\n",
        "        images.append(image)\n",
        "        labels.append('daruchini')\n",
        "        \n",
        "for i in tulsi_dir:\n",
        "    image = cv2.imread(fp_tulsi+i)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image = cv2.resize(image, (128,128))\n",
        "    images.append(image)\n",
        "    labels.append('tulsi')\n",
        "\n",
        "for i in tejpata_dir:\n",
        "    image = cv2.imread(fp_tejpata+i)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image = cv2.resize(image, (128,128))\n",
        "    images.append(image)\n",
        "    labels.append('tejpata')\n",
        "\n",
        "for i in niim_dir:\n",
        "    image = cv2.imread(fp_niim+i)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image = cv2.resize(image, (128,128))\n",
        "    images.append(image)\n",
        "    labels.append('niim')\n",
        "for i in shojne_dir:\n",
        "    image = cv2.imread(fp_shojne+i)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image = cv2.resize(image, (128,128))\n",
        "    images.append(image)\n",
        "    labels.append('shojne')\n",
        "for i in pathorkochi_dir:\n",
        "    image = cv2.imread(fp_pathorkochi+i)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image = cv2.resize(image, (128,128))\n",
        "    images.append(image)\n",
        "    labels.append('pathorkochi')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdDUFBRl-ka2"
      },
      "source": [
        "import pickle\n",
        "\n",
        "with open('/content/drive/MyDrive/Paper/images.txt', 'wb') as fh:\n",
        "   pickle.dump(images, fh)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQTHYD9vFOmE"
      },
      "source": [
        "import pickle\n",
        "pickle_off = open(\"/content/drive/MyDrive/Paper/images.txt\", 'rb')\n",
        "images = pickle.load(pickle_off)\n",
        "images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trnIxi0qHD7j"
      },
      "source": [
        "images.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RURksFawD0B1"
      },
      "source": [
        "plt.imshow(images[100])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQZCeiumD8uU"
      },
      "source": [
        "lb = LabelEncoder()\n",
        "labels = lb.fit_transform(labels)\n",
        "labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mD_9KoTrcGtk"
      },
      "source": [
        "lb.classes_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZa13StDEKtO"
      },
      "source": [
        "import pickle\n",
        "\n",
        "with open('/content/drive/MyDrive/Paper/labels128.txt', 'wb') as th:\n",
        "   pickle.dump(labels, th)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lo-PjksXFYor"
      },
      "source": [
        "import pickle\n",
        "pickle_off = open(\"/content/drive/MyDrive/Paper/labels.txt\", 'rb')\n",
        "labels = pickle.load(pickle_off)\n",
        "labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ln5z4Ut_IVy1"
      },
      "source": [
        "images = np.array(images)\n",
        "labels = np.array(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdjzofmoENsZ"
      },
      "source": [
        "(trainX,testX,trainY,testY) = train_test_split(images,labels,test_size=0.2,stratify=labels,random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsZjh5hpEecu"
      },
      "source": [
        "testY"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLIRg8Pv9hoD"
      },
      "source": [
        "plt.imshow(images[100])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoSS-c5SEcJ3"
      },
      "source": [
        "print(trainX.shape)\n",
        "print(type(trainX))\n",
        "print(trainY.shape)\n",
        "print(type(trainY))\n",
        "print(testX.shape)\n",
        "print(type(testX))\n",
        "print(testY.shape)\n",
        "print(type(testY))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggXgeynrElPQ"
      },
      "source": [
        "trainAug = ImageDataGenerator(\n",
        "    rotation_range=30,\n",
        "    zoom_range=0.15,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.15,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode=\"nearest\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7JA8LHjEoQt"
      },
      "source": [
        "valAug = ImageDataGenerator()\n",
        "mean = np.array([123.68, 116.779, 103.939], dtype=\"float32\")\n",
        "trainAug.mean = mean\n",
        "valAug.mean = mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoVNilxhDtIk"
      },
      "source": [
        " \n",
        "# import the libraries as shown below\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Lambda, Dense, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Conv2D, MaxPool2D, ZeroPadding2D, Dropout, BatchNormalization\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        " \n",
        "from tensorflow.keras.optimizers import Adam\n",
        " \n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTn6GbeaDIrG"
      },
      "source": [
        "model = Sequential()\n",
        " \n",
        "model.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu', input_shape = (224,224, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPool2D(2,2))\n",
        " \n",
        "model.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPool2D(2,2))\n",
        " \n",
        " \n",
        "model.add(Conv2D(filters=128, kernel_size=(3,3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPool2D(2,2))\n",
        " \n",
        " \n",
        " \n",
        "model.add(Flatten())\n",
        "model.add(Dense(226, activation='relu', ))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        " \n",
        "model.add(Dense(6, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nrp_C5MQD9iR"
      },
      "source": [
        "from keras.optimizers import Adam,RMSprop,SGD,Nadam,Adamax,Adadelta,Adagrad\n",
        "opt = Adagrad()\n",
        "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KGl40NrEFIn"
      },
      "source": [
        "BS = 32\n",
        "EPOCHS =50\n",
        "H = model.fit_generator(\n",
        "    trainAug.flow(trainX, trainY, batch_size=BS),\n",
        "    steps_per_epoch=len(trainX) // BS,\n",
        "    validation_data=valAug.flow(testX, testY),\n",
        "    validation_steps=len(testX) // BS,\n",
        "    epochs=EPOCHS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0eDuAaElkzQ"
      },
      "source": [
        " \n",
        "plt.plot(H.history['acc'])\n",
        "plt.plot(H.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.style.use(\"seaborn-darkgrid\")\n",
        "plt.title(\"Optimizer = Adagrad\")\n",
        "plt.show()\n",
        "# \"Loss\"\n",
        "plt.plot(H.history['loss'])\n",
        "plt.plot(H.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        " \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yYKiZGOvLyH"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(25,7))\n",
        "# Train\n",
        "sns.barplot(data = pd.DataFrame.from_dict([get_class_distribution(trainY)]).melt(), x = \"variable\", y=\"value\", hue=\"variable\",  ax=axes[0]).set_title('Class Distribution in Train Set')\n",
        "# Validation\n",
        "sns.barplot(data = pd.DataFrame.from_dict([get_class_distribution(testY)]).melt(), x = \"variable\", y=\"value\", hue=\"variable\",  ax=axes[1]).set_title('Class Distribution in Test Set')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8i-vO2vnX5Q"
      },
      "source": [
        "L = 5\n",
        "W = 5\n",
        "\n",
        "fig,axes = plt.subplots(L,W,figsize = (12,12))\n",
        "axes = axes.ravel()\n",
        "for i in np.arange(0,L*W):\n",
        "    axes[i].imshow(testX[i])\n",
        "    axes[i].set_title('Prediction Class = {:0.1f}\\n True Class = {:0.1f}'.format(Y_pred_classes[i],testY[i]))\n",
        "    axes[i].axis('off')\n",
        "plt.subplots_adjust(wspace = 0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHM7-Dh_pl1t"
      },
      "source": [
        "Y_pred = model.predict(testX)\n",
        "# Convert predictions classes to one hot vectors \n",
        "Y_pred_classes = np.argmax(Y_pred,axis = 1) \n",
        "# Convert validation observations to one hot vectors\n",
        "Y_true = np.argmax(testY,axis = 1) \n",
        "# compute the confusion matrix\n",
        "cm = confusion_matrix(Y_true, Y_pred_classes) \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzHoZFeco0s5"
      },
      "source": [
        "\n",
        "# Errors are difference between predicted labels and true labels\n",
        "errors = (Y_pred_classes - Y_true != 0)\n",
        "\n",
        "Y_pred_classes_errors = Y_pred_classes[errors]\n",
        "Y_pred_errors = Y_pred[errors]\n",
        "Y_true_errors = Y_true[errors]\n",
        "X_val_errors = testX[errors]\n",
        "\n",
        "def display_errors(errors_index,img_errors,pred_errors, obs_errors):\n",
        "    \"\"\" This function shows 6 images with their predicted and real labels\"\"\"\n",
        "    n = 0\n",
        "    nrows = 2\n",
        "    ncols = 3\n",
        "    fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True)\n",
        "    for row in range(nrows):\n",
        "        for col in range(ncols):\n",
        "            error = errors_index[n]\n",
        "            ax[row,col].imshow((img_errors[error]).reshape((32,32)))\n",
        "            ax[row,col].set_title(\"Predicted label :{}\\nTrue label :{}\".format(pred_errors[error],obs_errors[error]))\n",
        "            n += 1\n",
        "\n",
        "# Probabilities of the wrong predicted numbers\n",
        "Y_pred_errors_prob = np.max(Y_pred_errors,axis = 1)\n",
        "\n",
        "# Predicted probabilities of the true values in the error set\n",
        "true_prob_errors = np.diagonal(np.take(Y_pred_errors, Y_true_errors, axis=1))\n",
        "\n",
        "# Difference between the probability of the predicted label and the true label\n",
        "delta_pred_true_errors = Y_pred_errors_prob - true_prob_errors\n",
        "\n",
        "# Sorted list of the delta prob errors\n",
        "sorted_dela_errors = np.argsort(delta_pred_true_errors)\n",
        "\n",
        "# Top 6 errors \n",
        "most_important_errors = sorted_dela_errors[-6:]\n",
        "\n",
        "# Show the top 6 errors\n",
        "display_errors(most_important_errors, X_val_errors, Y_pred_classes_errors, Y_true_errors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsXii5ayvIVU"
      },
      "source": [
        "#evaluate the network\n",
        "predictions=model.predict(x=testX.astype(\"float32\"),batch_size=32)\n",
        "print(\"Classification report\",classification_report(testY.argmax(axis=1),predictions.argmax(axis=1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_jQl5NfvYWs"
      },
      "source": [
        "\n",
        "plt.plot(H.history['acc'])\n",
        "plt.plot(H.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.style.use(\"seaborn-darkgrid\")\n",
        "plt.title(\"Optimizer = Adam\")\n",
        "plt.show()\n",
        "# \"Loss\"\n",
        "plt.plot(H.history['loss'])\n",
        "plt.plot(H.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjJyEbXMAUBK"
      },
      "source": [
        "#evaluate the network\n",
        "predictions=model.predict(x=testX.astype(\"float32\"),batch_size=32)\n",
        "print(\"Classification report\",classification_report(testY.argmax(axis=1),predictions.argmax(axis=1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GNAKYIDAaIw"
      },
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "#model.save('cnnmodel.h5')  # creates a HDF5 file 'my_model.h5'\n",
        "model = load_model('cnnmodel.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IXKdMVCOTGD"
      },
      "source": [
        "cm = confusion_matrix(y_true=testY, y_pred=np.argmax(predictions, axis=-1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWQLdMcSEAj0"
      },
      "source": [
        "\n",
        "# Function to plot confusion matrix    \n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "# Predict the values from the validation dataset\n",
        "Y_pred = model.predict(testX)\n",
        "# Convert predictions classes to one hot vectors \n",
        "Y_pred_classes = np.argmax(Y_pred,axis = 1) \n",
        "# Convert validation observations to one hot vectors\n",
        "Y_true = np.argmax(testY,axis = 1) \n",
        "# compute the confusion matrix\n",
        "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes)\n",
        "\n",
        " \n",
        "\n",
        "# plot the confusion matrix\n",
        "plot_confusion_matrix(confusion_mtx, classes = lb.classes_) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czEI3brzNmUc"
      },
      "source": [
        "cm_plot_labels = ['Daruchini','Niim','Pathorkochi','Shojne','Tejpata','Tulsi']\n",
        "plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='Confusion Matrix')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YI6Ls3PQOpO"
      },
      "source": [
        "label_frac_error = 1 - np.diag(confusion_mtx) / np.sum(confusion_mtx, axis=1)\n",
        "plt.bar(np.arange(6),label_frac_error)\n",
        "plt.xlabel('True Label')\n",
        "plt.ylabel('Fraction classified incorrectly')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3R1qjaSDOLqq"
      },
      "source": [
        "\n",
        "# make a prediction for a new image.\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.models import load_model\n",
        " \n",
        "# load and prepare the image\n",
        "def load_image(filename):\n",
        "\t# load the image\n",
        "\timg = load_img(filename, target_size=(224, 224))\n",
        "\t# convert to array\n",
        "\timg = img_to_array(img)\n",
        "\t# reshape into a single sample with 3 channels\n",
        "\timg = img.reshape(1, 224, 224, 3)\n",
        "\t# prepare pixel data\n",
        "\timg = img.astype('float32')\n",
        "\timg = img / 255.0\n",
        "\treturn img\n",
        " \n",
        "# load an image and predict the class\n",
        "def run_example():\n",
        "\t# load the image\n",
        "\timg = load_image('img.jpg')\n",
        "\t# load model\n",
        "\t\n",
        "\t# predict the class\n",
        "\tresult = model.predict_classes(img)\n",
        "\tprint(result[0])\n",
        " \n",
        "# entry point, run the example\n",
        "run_example()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfUyyktfSsdx"
      },
      "source": [
        "labels.classess_\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5dvPDmKTHBc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}